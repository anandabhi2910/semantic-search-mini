{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0a02bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anand\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\anand\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228fa189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "611dd991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans a string by removing HTML tags, non-alphabetic characters,\n",
    "    and extra spaces, then converts it to lowercase.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text string.\n",
    "    \"\"\"\n",
    "    # Remove HTML tags (similar to Project 1)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Keep letters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra spaces and strip leading/trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def read_and_preprocess_documents(directory_path):\n",
    "    \"\"\"\n",
    "    Reads all text files from a directory, cleans the content, and\n",
    "    returns a list of cleaned document strings.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path to the directory containing the text files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of preprocessed document strings.\n",
    "    \"\"\"\n",
    "    cleaned_documents = []\n",
    "    # Use glob to find all files ending with .txt or .md\n",
    "    file_paths = glob.glob(os.path.join(directory_path, '*.txt')) + \\\n",
    "                 glob.glob(os.path.join(directory_path, '*.md'))\n",
    "    \n",
    "      # Add this line to see exactly what glob found\n",
    "    print(f\"Files found by glob: {file_paths}\")\n",
    "\n",
    "    if not file_paths:\n",
    "        print(f\"No text files found in the directory: {directory_path}\")\n",
    "        return []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        print(f\"Reading and cleaning file: {os.path.basename(file_path)}\")\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            raw_text = file.read()\n",
    "            cleaned_text = clean_text(raw_text)\n",
    "            cleaned_documents.append(cleaned_text)\n",
    "\n",
    "    print(f\"Successfully processed {len(cleaned_documents)} documents.\")\n",
    "    return cleaned_documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba256adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found by glob: ['c:\\\\Users\\\\anand\\\\Desktop\\\\semantic_search_mini\\\\data\\\\raw\\\\AIntel.txt', 'c:\\\\Users\\\\anand\\\\Desktop\\\\semantic_search_mini\\\\data\\\\raw\\\\ComputerVision.txt', 'c:\\\\Users\\\\anand\\\\Desktop\\\\semantic_search_mini\\\\data\\\\raw\\\\DL.txt', 'c:\\\\Users\\\\anand\\\\Desktop\\\\semantic_search_mini\\\\data\\\\raw\\\\MachineLearning.txt', 'c:\\\\Users\\\\anand\\\\Desktop\\\\semantic_search_mini\\\\data\\\\raw\\\\NLP.txt']\n",
      "Reading and cleaning file: AIntel.txt\n",
      "Reading and cleaning file: ComputerVision.txt\n",
      "Reading and cleaning file: DL.txt\n",
      "Reading and cleaning file: MachineLearning.txt\n",
      "Reading and cleaning file: NLP.txt\n",
      "Successfully processed 5 documents.\n",
      "\n",
      "--- Sample of Processed Document ---\n",
      "artificial intelligence ai is technology that enables computers and machines to simulate human learning comprehension problem solving decision making creativity and autonomy applications and devices equipped with ai can see and identify objects they can understand and respond to human language they can learn from new information and experience they can make detailed recommendations to users and experts they can act independently replacing the need for human intelligence or intervention a classic\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    # This approach works in notebooks and scripts by getting the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    RAW_DATA_PATH = os.path.join(current_dir, 'data', 'raw')\n",
    "    # Read and preprocess the documents\n",
    "    documents = read_and_preprocess_documents(RAW_DATA_PATH)\n",
    "\n",
    "    # Display a sample of the processed output\n",
    "    if documents:\n",
    "        print(\"\\n--- Sample of Processed Document ---\")\n",
    "        print(documents[0][:500]) # Print the first 500 characters of the first document\n",
    "        print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ea89b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
